# -*- coding: utf-8 -*-
"""Copy of prompt_test2_new

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lIFXTuzlcgLUVnHuw02GsAmUiNfCI_Ck
"""

!pip install firebase-admin

pip install chromadb openai sentence-transformers

pip install openai

!pip install -U sentence-transformers

from google.colab import files
from openai import OpenAI
import datetime
import random
from sentence_transformers import SentenceTransformer, util
import torch
from sklearn.cluster import KMeans
import numpy as np

import math
import datetime
from typing import List, Tuple

uploaded = files.upload()

import firebase_admin
from firebase_admin import credentials
from firebase_admin import firestore

# Path to your service account key file
# Make sure the filename matches what you uploaded
SERVICE_ACCOUNT_KEY_PATH = 'training-bd0a3-firebase-adminsdk-fbsvc-a47acf6ecd.json' # REPLACE with your actual file name

# Initialize the Firebase app
try:
    cred = credentials.Certificate(SERVICE_ACCOUNT_KEY_PATH)
    firebase_admin.initialize_app(cred)
    print("Firebase app initialized successfully!")
except Exception as e:
    print(f"Error initializing Firebase app: {e}")

# Get a Firestore client
db = firestore.client()

# --- Example: Writing data to Firestore ---
doc_ref = db.collection('colab_data').document('my_first_document')
doc_ref.set({
    'name': 'Firebase Fun',
    'version': 1.0,
    'from_colab': True,
    'timestamp': firestore.SERVER_TIMESTAMP
})
print("Data written to Firestore!")

# --- Example: Reading data from Firestore ---
doc = db.collection('colab_data').document('my_first_document').get()
if doc.exists:
    print(f"Document data: {doc.to_dict()}")
else:
    print("No such document!")

# --- Example: Querying a collection ---
docs = db.collection('colab_data').stream()
print("\nAll documents in 'colab_data' collection:")
for doc in docs:
    print(f"{doc.id} => {doc.to_dict()}")

# Reference a specific document within a collection (e.g., 'London' in 'cities')
doc_ref = db.collection('cities').document('London')

# Set data for the 'London' document
doc_ref.set({
    'name': 'London',
    'country': 'UK',
    'population': 8982000,
    'capital': True
})

print("Set document 'London' in 'cities' collection.")

# You can also use set() to update an existing document,
# or create it if it doesn't exist.
# Let's say we want to add more detail to 'Los Angeles'
la_doc_ref = db.collection('cities').document('Los Angeles')
la_doc_ref.set({
    'continent': 'North America',
    'area_sq_mi': 503
}, merge=True) # The 'merge=True' option is important here!

print("Updated 'Los Angeles' with merge: added continent and area.")

initial_variants = [
    "Summarize revenue trends using YoY and QoQ growth metrics.",
    "Write a concise paragraph highlighting revenue drivers",
    "List key revenue risks and growth factors in bullet points",
    "Summarize working capital swings and red flags clearly",
    "Give a financial due dilligence-style overview of revenue performance"

]


def upload_variants():
  for i, variant in enumerate(initial_variants):
    doc_id = f"variant_{i+1}"

    db.colletion("prompt_variants").document(doc_id).set({
        "text": variant,
        "source": "initial",
        "created_at": datetime.utcnow().isoformat(),
    })

    db.collection("ucb_stats").document(doc_id).set({
        "coyun"



    })

import openai
import chromadb
import numpy as np

# Set your OpenAI API key
openai.api_key = "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"

# Initialize the OpenAI client
client = openai.OpenAI(api_key=openai.api_key)

# ========== Step 1: Embed Prompts ==========
def embed_prompt(prompt: str) -> list:
    response = client.embeddings.create(
        model="text-embedding-3-small",
        input=prompt
    )
    return response.data[0].embedding

# ========== Step 2: Initialize ChromaDB ==========
chroma_client = chromadb.Client()
collection = chroma_client.get_or_create_collection("prompt_memory")

# ========== Step 3: Add Prompt + Reward ==========
def add_prompt_to_memory(prompt: str, reward: float, prompt_id: str = None):
    embedding = embed_prompt(prompt)
    if not prompt_id:
        prompt_id = f"prompt_{hash(prompt)}"
    collection.add(
        documents=[prompt],
        metadatas=[{"reward": reward}],
        ids=[prompt_id],
        embeddings=[embedding]
    )

# ========== Step 4: Retrieve Nearest Neighbors ==========
def get_similar_prompts(prompt: str, top_k: int = 5):
    query_embedding = embed_prompt(prompt)
    results = collection.query(
        query_embeddings=[query_embedding],
        n_results=top_k
    )
    return results['documents'][0], results['metadatas'][0], results['distances'][0]

# ========== Step 5: Weighted Reward Estimation ==========
def estimate_reward_from_neighbors(prompt: str, top_k: int = 5):
    _, metadatas, distances = get_similar_prompts(prompt, top_k)
    rewards = [meta['reward'] for meta in metadatas]
    # Add a small epsilon to distances to avoid division by zero or very large weights
    weights = np.exp(-np.array(distances) + 1e-8)  # closer = higher weight
    estimated_reward = np.average(rewards, weights=weights)
    return estimated_reward, rewards, distances

# ========== Step 6: UCB Prompt Selection (Example) ==========
import math
from time import time

def select_best_prompt_ucb(candidate_prompts, t, c=1.0):
    best_score = float("-inf")
    best_prompt = None

    for prompt in candidate_prompts:
        est_reward, _, _ = estimate_reward_from_neighbors(prompt)
        # UCB-style bonus term (assuming frequency = 1 to avoid div/0)
        bonus = c * math.sqrt(math.log(t + 1))
        score = est_reward + bonus

        if score > best_score:
            best_score = score
            best_prompt = prompt

    return best_prompt

# ========== Example Usage ==========
if __name__ == "__main__":
    # Add a few prompt examples and rewards
    add_prompt_to_memory("Summarize EBITDA trends in Q2", reward=0.9)
    add_prompt_to_memory("Highlight revenue changes by segment", reward=0.7)
    add_prompt_to_memory("Show key growth drivers from 2022", reward=0.8)

    # Estimate reward for a new prompt
    test_prompt = "Analyze EBITDA performance during Q2"
    reward, _, _ = estimate_reward_from_neighbors(test_prompt)
    print(f"Estimated reward: {reward:.3f}")

    # Choose best prompt from candidates
    candidates = [
        "Summarize EBITDA performance in Q2",
        "Give me a breakdown of revenue streams",
        "Explain the cost structure for Q3"
    ]
    chosen = select_best_prompt_ucb(candidates, t=10)
    print("Selected prompt:", chosen)

class FirebaseUCB:
    def __init__(self, db):
        self.db = db
        self.variant_ids, self.variants = self.load_variants()
        self.counts, self.values, self.total_count = self.load_ucb_stats()

    def load_variants(self) -> Tuple[List[str], List[str]]:
        prompt_docs = self.db.collection("prompt_variations").stream()
        variant_ids = []
        variants = []
        for doc in prompt_docs:
            variant_ids.append(doc.id)
            variants.append(doc.to_dict().get("text", ""))
        return variant_ids, variants

    def load_ucb_stats(self):
        counts = []
        values = []
        total_count = 0
        for doc_id in self.variant_ids:
            stat_ref = self.db.collection("ucb_stats").document(doc_id)
            doc = stat_ref.get()
            if doc.exists:
                stat = doc.to_dict()
                count = stat.get("count", 0)
                value = stat.get("avg_reward", 0.0)
            else:
                count = 0
                value = 0.0
            counts.append(count)
            values.append(value)
            total_count += count
        return counts, values, total_count

    def select_variant(self) -> Tuple[int, str, str]:
        # Prioritize variants with count 0
        for i, count in enumerate(self.counts):
            if count == 0:
                return i, self.variants[i], self.variant_ids[i]

        # Check if there are any variants left after filtering
        if not self.variants:
            print("Error: No prompt variants available for UCB calculation.")
            return None, None, None  # Indicate no variant was selected

        ucb_values = [
            self.values[i] + math.sqrt((2 * math.log(self.total_count)) / self.counts[i])
            for i in range(len(self.counts))
        ]
        selected_index = ucb_values.index(max(ucb_values))
        return selected_index, self.variants[selected_index], self.variant_ids[selected_index]

    def update(self, variant_index: int, reward: float):
        variant_id = self.variant_ids[variant_index]
        count = self.counts[variant_index]
        value = self.values[variant_index]

        new_count = count + 1
        new_value = ((count * value) + reward) / new_count

        self.counts[variant_index] = new_count
        self.values[variant_index] = new_value
        self.total_count += 1

        self.db.collection("ucb_stats").document(variant_id).set({
            "count": new_count,
            "avg_reward": new_value,
            "updated_at": datetime.datetime.utcnow().isoformat()
        }, merge=True)

def embeddings_filter(prompt_variations_txt, prompt_variations_id, threshold):

  prompt_variations_id_filtered = []

  model = SentenceTransformer('all-MiniLM-L6-v2')
  embeddings = model.encode(prompt_variations_txt, convert_to_tensor=True)
  cosine_sim_matrix = util.pytorch_cos_sim(embeddings, embeddings)

  keep = []
  seen = set()

  for i in range(len(prompt_variations_txt)):
      if i in seen:
          continue

      keep.append(prompt_variations_txt[i])

      # Compare i with all other prompts
      for j in range(i + 1, len(prompt_variations_txt)):
          if cosine_sim_matrix[i][j] > threshold:
              seen.add(j)


  for i in range(len(prompt_variations_txt)):
    if prompt_variations_txt[i] in keep:
      prompt_variations_id_filtered.append(prompt_variations_id[i])


  return keep, prompt_variations_id_filtered

def clustering_filter(prompt_variations_txt, prompt_variations_id, n_clusters=5):
    model = SentenceTransformer('all-MiniLM-L6-v2')
    embeddings = model.encode(prompt_variations_txt)

    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    labels = kmeans.fit_predict(embeddings)

    selected_prompts = []
    selected_ids = []

    for label in set(labels):
        idx = list(labels).index(label)  # take the first prompt from each cluster
        selected_prompts.append(prompt_variations_txt[idx])
        selected_ids.append(prompt_variations_id[idx])

    return selected_prompts, selected_ids

general_modifiers = [
    "Write in a concise, professional tone suitable for a client-facing FDD.",
    "Organize the output using headings and bullet points.",
    "Include relevant KPIs and use numerical examples where possible.",
    "Highlight key drivers, trends, and anomalies in the analysis.",
    "Focus on what the data implies, not just what it says.",
    "Compare performance across time periods (e.g., QoQ or YoY) if applicable.",
    "Emphasize points that would be material to a potential investor or acquirer.",
    "Identify any financial red flags or unusual swings.",
    "Base all conclusions on observable facts or metrics.",
    "Match the tone and formatting of a financial due diligence report."
]

categories = {
        "Financial Analysis": ["ebitda", "margin", "profit", "cash flow", "income statement", "financial analysis", "balance sheet"],
        "Valuation & Modeling": ["dcf", "valuation", "wacc", "multiple", "terminal value", "comparable"],
        "Operational Analysis / KPIs": ["cost structure", "headcount", "supply chain", "efficiency", "kpi", "throughput"],
        "Commercial Due Diligence": ["market size", "tam", "competition", "pricing", "swot", "customer", "segment"],
        "Executive Summary / Report Drafting": ["executive summary", "conclusion", "key findings", "recommendation", "summary"],
        "Client Emails / Communications": ["email", "client", "response", "follow-up", "update", "message"],
        "Slides / Bullet Creation": ["slide", "bullet", "deck", "powerpoint", "visual", "format"],
        "Restructuring / Turnaround": ["restructuring", "liquidity", "cash runway", "turnaround", "burn rate", "cost cutting"],
        "HR / Culture / Interview": ["employee", "culture", "interview", "retention", "recruit", "staff", "hr"],
        "General Consulting / Strategy": ["benchmark", "go-to-market", "strategy", "market entry", "competitive"]
    }


category_prototypes = {
    "Financial Analysis": "Analyze EBITDA margin and cash flow trends over the last fiscal year.",
    "Valuation & Modeling": "Perform a DCF valuation and calculate enterprise value using comparables.",
    "Operational Analysis / KPIs": "Evaluate cost drivers and assess KPI performance in operations.",
    "Commercial Due Diligence": "Summarize market sizing and pricing trends in competitive landscape.",
    "Executive Summary / Report Drafting": "Write a concise executive summary of the key findings.",
    "Client Emails / Communications": "Write a client-facing email summarizing project status.",
    "Slides / Bullet Creation": "Generate slide bullets on headcount reduction strategies.",
    "Restructuring / Turnaround": "Summarize a short-term liquidity improvement plan.",
    "HR / Culture / Interview": "Explain how to improve employee retention in a consulting firm.",
    "General Consulting / Strategy": "Draft a go-to-market recommendation for a new market entry."
}

def embeddings_categorization(prompt):
  model = SentenceTransformer("all-MiniLM-L6-v2")

  category_embeddings = {
    cat: model.encode(text, convert_to_tensor=True)
    for cat, text in category_prototypes.items()
    }

  prompt_emb = model.encode(prompt, convert_to_tensor = True)
  scores = {
    cat: util.cos_sim(prompt_emb, emb)[0].item()
    for cat, emb in category_embeddings.items()
  }

  return max(scores, key=scores.get)

def categorize_prompt(prompt):
    # Removed the redefinition of categories here to use the global categories dictionary
    categories_matched = []

    # Initialize categories_counter with zeros for each category in the global dictionary
    categories_counter = [0] * len(categories)


    prompt_lower = prompt.lower()
    # Iterate through the global categories dictionary
    for i, (category, keywords) in enumerate(categories.items()):
        for keyword in keywords:
            if keyword in prompt_lower:
              categories_counter[i] += 1

    for i in range(len(categories)):
      if categories_counter[i] > 0: # Check if any keyword matched for the category
        categories_matched.append(list(categories.keys())[i]) # Get the category name by index

    if len(categories_matched) == 1:
      return categories_matched[0] # Return the single matched category
    else:
      # If multiple or no categories matched, use embeddings categorization
      category = embeddings_categorization(prompt)
      return category

from firebase_admin import firestore
from sentence_transformers import SentenceTransformer
import numpy as np
import datetime
import random

# --- Embedding Setup ---
model = SentenceTransformer("all-MiniLM-L6-v2")  # Replace with your preferred embedding model

class SemanticEmbeddingStore:
    def __init__(self, db):
        self.db = db

    def store_embedding(self, prompt_id, text):
        embedding = model.encode(text).tolist()
        self.db.collection("prompt_embeddings").document(prompt_id).set({
            "embedding": embedding
        })

    def get_embedding(self, prompt_id):
        doc = self.db.collection("prompt_embeddings").document(prompt_id).get()
        return np.array(doc.to_dict()["embedding"]) if doc.exists else None

    def get_all_embeddings(self):
        prompts = []
        ids = []
        embeddings = []
        for doc in self.db.collection("prompt_embeddings").stream():
            d = doc.to_dict()
            if "embedding" in d:
                prompts.append(doc.id)
                ids.append(doc.id)
                embeddings.append(np.array(d["embedding"]))
        return ids, embeddings


class SemanticUCB:
    def __init__(self, db, weight=0.7):
        self.db = db
        self.weight = weight
        self.embedding_store = SemanticEmbeddingStore(db)

    def similarity(self, emb1, emb2):
        return np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))

    def select_variant(self, user_prompt):
        user_embedding = model.encode(user_prompt)

        stats = list(self.db.collection("ucb_stats").stream())
        prompt_ids = [doc.id for doc in stats]

        max_score = -float("inf")
        selected_id = None
        selected_prompt = None
        selected_index = -1

        for i, doc in enumerate(stats):
            stat = doc.to_dict()
            count = stat.get("count", 0)
            avg_reward = stat.get("avg_reward", 0.0)

            if count == 0:
                ucb_score = 1.0  # Favor exploration
            else:
                ucb_score = avg_reward + np.sqrt(2 * np.log(len(stats)) / count)

            emb = self.embedding_store.get_embedding(stat["prompt_id"])
            sim_score = self.similarity(user_embedding, emb) if emb is not None else 0.0

            final_score = self.weight * ucb_score + (1 - self.weight) * sim_score

            if final_score > max_score:
                max_score = final_score
                selected_id = stat["prompt_id"]
                selected_prompt = self.db.collection("prompt_variations").document(selected_id).get().to_dict()["text"]
                selected_index = i

        return selected_index, selected_prompt, selected_id

    def update(self, prompt_id, feedback):
        doc_ref = self.db.collection("ucb_stats").document(prompt_id)
        stat = doc_ref.get().to_dict()

        count = stat.get("count", 0)
        avg_reward = stat.get("avg_reward", 0.0)

        new_count = count + 1
        new_avg = (avg_reward * count + feedback) / new_count

        doc_ref.update({
            "count": new_count,
            "avg_reward": new_avg
        })


# --- Main Loop ---
ucb = SemanticUCB(db)
embedding_store = SemanticEmbeddingStore(db)

while True:
    user_prompt = input("What can I help you with today? ")
    category = categorize_prompt(user_prompt)
    c = 0
    prompts_read = []
    prompt_variations_txt = []
    prompt_variations_id = []

    doc_id = f"user_prompt_{c}"

    db.collection("prompt_variations").document(doc_id).set({
        "text": user_prompt,
        "source": "user",
        "category": category,
        "created_at": datetime.datetime.utcnow().isoformat()
    })

    db.collection("ucb_stats").document(doc_id).set({
        "prompt_id": doc_id,
        "count": 0,
        "avg_reward": 0.0,
        "category": category,
        "created_at": datetime.datetime.utcnow().isoformat()
    })

    embedding_store.store_embedding(doc_id, user_prompt)

    variation = "Your job is to act as a senior financial due diligence professional..."

    for i in range(10):
        c += 1
        doc_ref = db.collection("previous_prompts").document("last_prompt")
        doc = doc_ref.get()
        guidance = variation
        if doc.exists:
            guidance += "\n\nPrevious: " + doc.to_dict().get("text", "")

        '''response = client.responses.create(
            model="gpt-4.1",
            input=user_prompt + guidance
        )'''

        doc_id = f"user_prompt_{c}"
        prompt_new = response.output_text
        category = categorize_prompt(prompt_new)

        db.collection("prompt_variations").document(doc_id).set({
            "text": prompt_new,
            "source": "user",
            "category": category,
            "created_at": datetime.datetime.utcnow().isoformat()
        })

        db.collection("ucb_stats").document(doc_id).set({
            "prompt_id": doc_id,
            "count": 0,
            "avg_reward": 0.0,
            "category": category,
            "created_at": datetime.datetime.utcnow().isoformat()
        })

        db.collection("previous_prompts").document("last_prompt").set({
            "text": prompt_new,
            "source": "user",
            "category": category,
            "created_at": datetime.datetime.utcnow().isoformat()
        })

        embedding_store.store_embedding(doc_id, prompt_new)
        prompt_variations_txt.append(prompt_new)
        prompt_variations_id.append(doc_id)

    filtered_prompts, filtered_ids = clustering_filter(prompt_variations_txt, prompt_variations_id, n_clusters=8)
    unwanted_ids = set(prompt_variations_id) - set(filtered_ids)

    for doc_id in unwanted_ids:
        db.collection("prompt_variations").document(doc_id).delete()
        db.collection("ucb_stats").document(doc_id).delete()
        db.collection("prompt_embeddings").document(doc_id).delete()

    print("\nüöÄ Selecting best prompt variant...")
    index, selected_prompt, doc_id = ucb.select_variant(user_prompt)

    if selected_prompt is not None:
        print(f"\nüìå Selected Prompt ID: {doc_id}")
        print(f"üîç Prompt: {selected_prompt}\n")

        response = client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": selected_prompt}]
        )
        print("üß† Response:\n", response.choices[0].message.content)

        while True:
            try:
                feedback = float(input("Rate the output quality from 0 (bad) to 1 (excellent): "))
                if 0.0 <= feedback <= 1.0:
                    break
                else:
                    print("‚ùó Please enter a number between 0 and 1.")
            except:
                print("‚ùó Invalid input.")

        ucb.update(doc_id, feedback)

    else:
        print("No prompt variant was selected.")

    cont = input("\nType 'q' to quit, anything else to continue: ")
    if cont.lower() == 'q':
        break